{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9462b1",
   "metadata": {},
   "source": [
    "## RAG Workflow \n",
    "This notebook demonstrates the basic workflow of Retrieval-Augmented Generation (RAG) for testing purposes.\n",
    "\n",
    "#### Steps included:\n",
    "1. Importing necessary libraries from LangChain, HuggingFace, and other tools.\n",
    "2. Loading PDF documents from a directory using LangChain's DirectoryLoader.\n",
    "3. Splitting the documents into overlapping text chunks for better retrieval.\n",
    "4. Creating vector embeddings of these chunks and storing them in a Chroma vector database.\n",
    "5. Setting up an LLM (Google Gemini) for answer generation based on retrieved chunks.\n",
    "6. Running retrieval QA to answer user queries based on the ingested documents.\n",
    "7. Benchmarking different text splitter chunk sizes for retrieval and similarity performance.\n",
    " \n",
    "The notebook is intended for experimentation and prototyping on academic PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ab302",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a748262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# Langchain Components\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cdf3d",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation\n",
    "- the data being loaded is PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642a6a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load all PDF documents from the data directory\n",
    "loader = DirectoryLoader(\n",
    "    path=\"../data\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bccf973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\BERT-Pre-training-of-Deep-Bidirectional-Transformers-paper.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and ﬁne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\narXiv:1810.04805v2  [cs.CL]  24 May 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\BERT-Pre-training-of-Deep-Bidirectional-Transformers-paper.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nﬁne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\BERT-Pre-training-of-Deep-Bidirectional-Transformers-paper.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='BERT BERT\\nE[CLS] E1  E[SEP]... EN E1’ ... EM’\\nC\\n T1\\n T[SEP]...\\n TN\\n T1’ ...\\n TM’\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]... EN E1’ ... EM’\\nC\\n T1\\n T[SEP]...\\n TN\\n T1’ ...\\n TM’\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nNERMNLI\\nFigure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-\\nmal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H = 768and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\BERT-Pre-training-of-Deep-Bidirectional-Transformers-paper.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ( [CLS]). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token as C ∈RH,\\nand the ﬁnal hidden vector for the ith input token\\nas Ti ∈RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the ﬁnal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nﬁne-tuning, since the [MASK] token does not ap-\\npear during ﬁne-tuning. To mitigate this, we do\\nnot always replace “masked” words with the ac-\\ntual [MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthe i-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTi will be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-\\ntionships, we pre-train for a binarized next sen-\\ntence prediction task that can be trivially gener-\\nated from any monolingual corpus. Speciﬁcally,\\nwhen choosing the sentencesA and B for each pre-\\ntraining example, 50% of the time B is the actual\\nnext sentence that follows A (labeled as IsNext),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext). As we show\\nin Figure 1, C is used for next sentence predic-\\ntion (NSP). 5 Despite its simplicity, we demon-\\nstrate in Section 5.1 that pre-training towards this\\ntask is very beneﬁcial to both QA and NLI. 6\\n5The ﬁnal model achieves 97%-98% accuracy on NSP.\\n6The vector C is not a meaningful sentence representation\\nwithout ﬁne-tuning, since it was trained with NSP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\BERT-Pre-training-of-Deep-Bidirectional-Transformers-paper.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='[CLS] he likes play ##ing [SEP]my dog is cute [SEP]Input\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP]Emy Edog Eis Ecute E[SEP]\\nToken\\nEmbeddings\\nEA EB EB EB EB EBEA EA EA EA EASegment\\nEmbeddings\\nE0 E6 E7 E8 E9 E10E1 E2 E3 E4 E5Position\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufﬂed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciﬁc inputs and outputs into BERT and ﬁne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence A and sentence B from pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and\\n(4) a degenerate text- ∅ pair in text classiﬁcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiﬁcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, ﬁne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model. 7 We de-\\nscribe the task-speciﬁc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT ﬁne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo ﬁne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the ﬁnal hid-\\nden vector C ∈ RH corresponding to the ﬁrst\\ninput token ([CLS]) as the aggregate representa-\\ntion. The only new parameters introduced during\\nﬁne-tuning are classiﬁcation layer weights W ∈\\nRK×H, where Kis the number of labels. We com-\\npute a standard classiﬁcation loss with C and W,\\ni.e., log(softmax(CWT )).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview loaded documents\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274b827",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store Creation\n",
    "- Type of Embedding model being used: `AAI/bge-base-en-v1.5`\n",
    "- Type of LLM being used: `Gemini 2.0 flash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316e05b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10592\\4227147074.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "# -- Set up Embeddding model --\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# -- Set up LLM model --\n",
    "# Load Gemini API Key\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables.\")\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.1)\n",
    "\n",
    "# -- Set up Question and Ground Truth Pairs --\n",
    "questions = [\n",
    "    \"What does BERT stand for?\",\n",
    "    \"What kind of attention mechanism is introduced in 'Attention Is All You Need'?\",\n",
    "    \"How is positional encoding implemented in the Transformer model?\",\n",
    "    \"What are the pre-training objectives used in BERT?\",\n",
    "    \"Compare the architecture of BERT and the Transformer model.\",\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"Bidirectional Encoder Representations from Transformers.\",\n",
    "    \"Self-attention mechanism and multi-head attention.\",\n",
    "    \"Positional encoding is implemented using sine and cosine functions.\",\n",
    "    \"BERT's pre-training objectives include masked language modeling and next sentence prediction.\",\n",
    "    \"BERT uses only the encoder part of the Transformer model.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7655081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build vector store\n",
    "def build_vector_store(docs, chunk_size, chunk_overlap):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    chroma = Chroma.from_documents(\n",
    "        chunks, \n",
    "        embeddings_model,\n",
    "        persist_directory=f\"../chroma_db/{chunk_size}_{chunk_overlap}\"\n",
    "    )\n",
    "\n",
    "    return chroma\n",
    "\n",
    "# Function to evaluate a single configuration\n",
    "def evaluate_config(docs, chunk_size, chunk_overlap, k=3):\n",
    "    print(f\"\\nEvaluating for chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "    vector_store = build_vector_store(docs, chunk_size, chunk_overlap)\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "\n",
    "    # -- Set up RAG Chain ---\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    # --- Evaluate the RAG Chain ---\n",
    "    results = []\n",
    "    for q, gt in zip(questions, ground_truths):\n",
    "        # Compute retrieval time\n",
    "        start = time.time()\n",
    "        result = rag_chain.invoke({\"query\": q})\n",
    "        end = time.time()\n",
    "\n",
    "        response = result[\"result\"]\n",
    "        docs = result[\"source_documents\"]\n",
    "\n",
    "        # Compute cosine similarity between response and ground truth\n",
    "        emb_pred = embeddings_model.embed_query(response)\n",
    "        emb_true = embeddings_model.embed_query(gt)\n",
    "        similarity = cosine_similarity([emb_pred], [emb_true])[0][0]\n",
    "\n",
    "        results.append({\n",
    "            \"question\": q,\n",
    "            \"ground_truth\": gt,\n",
    "            \"response\": response,\n",
    "            \"mean_similarity\": similarity,\n",
    "            \"retrieval_time\": round(end - start, 3)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    summary = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"avg_similarity\": np.mean(df[\"mean_similarity\"]),\n",
    "        \"avg_retrieval_time\": np.mean(df[\"retrieval_time\"])\n",
    "    }\n",
    "    print(pd.DataFrame([summary]))\n",
    "    return df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda8082",
   "metadata": {},
   "source": [
    "### Chunking Parameters Evaluation\n",
    "- Comparison for different chunking parameters like `chunk_size`, `chunk_overlap`\n",
    "- The comparison is made based on responses produced by LLM, mean retrieval time and mean similarity score\n",
    "- Top-k: `k=3` is chosen, which means only select the top 3 documents with the highest relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7fe276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating for chunk_size=200, overlap=0\n",
      "   chunk_size  chunk_overlap  avg_similarity  avg_retrieval_time\n",
      "0         200              0        0.764071              1.6798\n",
      "\n",
      "Evaluating for chunk_size=512, overlap=50\n",
      "   chunk_size  chunk_overlap  avg_similarity  avg_retrieval_time\n",
      "0         512             50        0.682983               1.522\n",
      "\n",
      "Evaluating for chunk_size=1024, overlap=100\n",
      "   chunk_size  chunk_overlap  avg_similarity  avg_retrieval_time\n",
      "0        1024            100        0.726011              1.3054\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    {\"chunk_size\": 200, \"chunk_overlap\": 0},\n",
    "    {\"chunk_size\": 512, \"chunk_overlap\": 50},\n",
    "    {\"chunk_size\": 1024, \"chunk_overlap\": 100}\n",
    "]\n",
    "\n",
    "all_details, summaries = [], []\n",
    "for cfg in config:\n",
    "    df, summary = evaluate_config(documents, cfg[\"chunk_size\"], cfg[\"chunk_overlap\"])\n",
    "    all_details.append(df)\n",
    "    summaries.append(summary)\n",
    "\n",
    "comparison_df = pd.DataFrame(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8de2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>avg_similarity</th>\n",
       "      <th>avg_retrieval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.764071</td>\n",
       "      <td>1.6798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>512</td>\n",
       "      <td>50</td>\n",
       "      <td>0.682983</td>\n",
       "      <td>1.5220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>100</td>\n",
       "      <td>0.726011</td>\n",
       "      <td>1.3054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_size  chunk_overlap  avg_similarity  avg_retrieval_time\n",
       "0         200              0        0.764071              1.6798\n",
       "1         512             50        0.682983              1.5220\n",
       "2        1024            100        0.726011              1.3054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95277cd",
   "metadata": {},
   "source": [
    "Based on the table above, we observed that:\n",
    "- chunk size: 200, chunk overlap: 0 having the highest similarity but longest retrieval time\n",
    "- chunk size: 50, chunk oevrlap: 50 having lowest similarity and average retrieval time\n",
    "- chunk size: 1024, chunk overlap: 100 having average similarity and fatest retrieval speed\n",
    "\n",
    "Next we can looks into the responses generated by each configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a161073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response for chunk size 200, overlap 0:\n",
      "Q: What does BERT stand for?\n",
      "R: Bidirectional Encoder Representations from\n",
      "\n",
      "Q: What kind of attention mechanism is introduced in 'Attention Is All You Need'?\n",
      "R: The paper \"Attention Is All You Need\" introduces the self-attention mechanism (sometimes called intra-attention).\n",
      "\n",
      "Q: How is positional encoding implemented in the Transformer model?\n",
      "R: The context states that positional encodings are added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "Q: What are the pre-training objectives used in BERT?\n",
      "R: The provided context mentions \"No NSP\" as a training objective used with the same pre-training data, fine-tuning scheme, and hyperparameters as BERTBASE. It also mentions that the BERT model is initialized with pre-trained parameters and fine-tuned using labeled data. However, the context doesn't explicitly list *all* the pre-training objectives used in BERT.\n",
      "\n",
      "Q: Compare the architecture of BERT and the Transformer model.\n",
      "R: The provided context doesn't offer a comparison of the architecture of BERT and the Transformer model. It only mentions that BERT is a fine-tuning based representation model that achieves state-of-the-art performance on various tasks and that it is applied to the CoNLL-2003 Named Entity Recognition (NER) task.\n",
      "\n",
      "\n",
      "Response for chunk size 512, overlap 50:\n",
      "Q: What does BERT stand for?\n",
      "R: I am sorry, but the provided text does not explicitly state what BERT stands for.\n",
      "\n",
      "Q: What kind of attention mechanism is introduced in 'Attention Is All You Need'?\n",
      "R: The context doesn't explicitly state what kind of attention mechanism is introduced in \"Attention Is All You Need.\" However, it does mention self-attention and lists the authors of the paper.\n",
      "\n",
      "Q: How is positional encoding implemented in the Transformer model?\n",
      "R: Positional encodings are added to the input embeddings.\n",
      "\n",
      "Q: What are the pre-training objectives used in BERT?\n",
      "R: The provided text states that during pre-training, the BERT model is trained on unlabeled data over different pre-training tasks. However, it doesn't specify what those pre-training tasks are.\n",
      "\n",
      "Q: Compare the architecture of BERT and the Transformer model.\n",
      "R: I don't know. The provided text discusses the differences between BERT, ELMo, and OpenAI GPT, and mentions that OpenAI GPT uses a Transformer LM. However, it doesn't directly compare BERT's architecture to the general Transformer model architecture.\n",
      "\n",
      "\n",
      "Response for chunk size 1024, overlap 100:\n",
      "Q: What does BERT stand for?\n",
      "R: The provided text doesn't explicitly state what BERT stands for.\n",
      "\n",
      "Q: What kind of attention mechanism is introduced in 'Attention Is All You Need'?\n",
      "R: The paper introduces a network architecture called the Transformer, based solely on attention mechanisms. It also discusses self-attention, sometimes called intra-attention.\n",
      "\n",
      "Q: How is positional encoding implemented in the Transformer model?\n",
      "R: Positional encodings are added to the input embeddings at the beginning of the encoder and decoder stacks.\n",
      "\n",
      "Q: What are the pre-training objectives used in BERT?\n",
      "R: The provided text states that during pre-training, the BERT model is trained on unlabeled data over different pre-training tasks. However, it doesn't explicitly list the specific pre-training objectives used.\n",
      "\n",
      "Q: Compare the architecture of BERT and the Transformer model.\n",
      "R: BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017). The BERT implementation is almost identical to the original Transformer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access first config (200, 0)\n",
    "df_200 = all_details[0]\n",
    "\n",
    "# Access second config (512, 50)\n",
    "df_512 = all_details[1]\n",
    "\n",
    "# Access third config (1024, 100)\n",
    "df_1024 = all_details[2]\n",
    "\n",
    "print(\"\\nResponse for chunk size 200, overlap 0:\")\n",
    "for q, ans in zip(df_200[\"question\"], df_200[\"response\"]):\n",
    "    print(f\"Q: {q}\\nR: {ans}\\n\")\n",
    "\n",
    "print(\"\\nResponse for chunk size 512, overlap 50:\")\n",
    "for q, ans in zip(df_512[\"question\"], df_512[\"response\"]):\n",
    "    print(f\"Q: {q}\\nR: {ans}\\n\")\n",
    "\n",
    "print(\"\\nResponse for chunk size 1024, overlap 100:\")\n",
    "for q, ans in zip(df_1024[\"question\"], df_1024[\"response\"]):\n",
    "    print(f\"Q: {q}\\nR: {ans}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
